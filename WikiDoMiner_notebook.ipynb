{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMtf2OPlzJZv"
      },
      "source": [
        "# Installation\n",
        "Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJT5vs9tkZxK"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia-api wikipedia PyPDF2==2.2.0\n",
        "!spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiGg0hWckRmI",
        "outputId": "d31ed63c-1457-4db5-97fe-41222e4108b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ],
      "source": [
        "import wikipediaapi, calendar, spacy, nltk, os, re, en_core_web_md\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download(['punkt','stopwords','wordnet','omw-1.4'])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from PyPDF2 import PdfReader\n",
        "import wikipedia as wiki\n",
        "wp = wikipediaapi.Wikipedia('en')\n",
        "nlp = en_core_web_md.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpIgl3opzNxm"
      },
      "source": [
        "# Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTsH-eiMzTk3"
      },
      "outputs": [],
      "source": [
        "def calculate_jaccard(text1,text2):  # calculates jaccard similarity between two string\n",
        "  word_tokens1=word_tokenize(text1.lower())\n",
        "  word_tokens2=word_tokenize(text2.lower())\n",
        "  both_tokens = word_tokens1 + word_tokens2\n",
        "  union = set(both_tokens)\n",
        "  # Calculate intersection.\n",
        "  intersection = set()\n",
        "  for w in word_tokens1:\n",
        "    if w in word_tokens2:\n",
        "      intersection.add(w)\n",
        "  jaccard_score = len(intersection)/len(union)\n",
        "  return jaccard_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4LscR8M6_Z5"
      },
      "outputs": [],
      "source": [
        "def stemlemma(text):\n",
        "  return ' '.join([stemmer.stem(wordnet_lemmatizer.lemmatize(word)) for word in word_tokenize(text.lower())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVo0WRd27C8k"
      },
      "outputs": [],
      "source": [
        "def openFiles(files,path):\n",
        "  li=[]\n",
        "  for f in files:\n",
        "    with open(path+f,\"r\") as tf:\n",
        "      li.append(tf.read().replace('\\n', ''))\n",
        "  return li"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbDFl1VR6yfr"
      },
      "source": [
        "## Extract Keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKeB6fbZzblF"
      },
      "outputs": [],
      "source": [
        "# get all nouns and noun phrases from the input sentence\n",
        "def getAllNPsFromSent(sent,include_nouns=False):\n",
        "    npstr=[]\n",
        "    chunks = list(sent.noun_chunks)\n",
        "    for i in range(len(chunks)):\n",
        "        np=chunks[i]\n",
        "        if len(np)==1:\n",
        "            if np[0].pos_!=\"NOUN\":\n",
        "                continue\n",
        "        if np.text.lower() not in npstr:\n",
        "            npstr.append(np.text.lower())      \n",
        "        if i < len(chunks)-1:\n",
        "            np1=chunks[i+1]\n",
        "            if np1.start-np.end==1:\n",
        "                if sent.doc[np.end].tag_==\"CC\":\n",
        "                    newnp = sent[np.start:np1.end]\n",
        "                    if newnp.text.lower() not in npstr:\n",
        "                        npstr.append(newnp.text.lower())\n",
        "    if include_nouns:\n",
        "        for t in sent:\n",
        "            if \"subj\" in t.dep_ and t.pos_==\"NOUN\": \n",
        "                if t.text.lower() not in npstr:\n",
        "                    npstr.append(t.text.lower())\n",
        "    return npstr   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_D8ve5lzjCn"
      },
      "outputs": [],
      "source": [
        "def getTopK(di,K=50):\n",
        "  tempdf=pd.DataFrame.from_dict(di,columns=[\"tfidf\"], orient='index')\n",
        "  return list(tempdf.sort_values(by=['tfidf'],ascending=False)[:K].index)\n",
        "\n",
        "def getKeywords(doc,nlp,include_nouns=False,tfidf=[],K=None): # K: free parameter\n",
        "  keywords=[]\n",
        "  for s in doc.split('\\n'):\n",
        "    s=nlp(s)\n",
        "    keywords.extend([n.text for n in list(s.ents)])\n",
        "    keywords.extend(list(getAllNPsFromSent(s,include_nouns)))\n",
        "  \n",
        "  keywords=list(set(keywords))\n",
        "\n",
        "  keywords_wn={}\n",
        "\n",
        "  if len(tfidf)>0:\n",
        "    tfidf_threshold=np.mean([t for t in tfidf if t>0])\n",
        "\n",
        "  for k in keywords:\n",
        "    keyword=' '.join([word for word in word_tokenize(k) if not word.lower() in stopwords.words('english')])\n",
        "    if not wn.synsets(keyword) and keyword.replace(' ','').isalpha() and not keyword.isupper() and not np.array([k.isupper() for k in [ky[:-1] for ky in keyword.split()]]).any():\n",
        "      keyword=keyword.lower()\n",
        "      if len(tfidf)>0:\n",
        "        if stemlemma(keyword) in tfidf.index:# and len(keyword)>2:\n",
        "          if tfidf[stemlemma(keyword)]>tfidf_threshold:\n",
        "            if keyword not in keywords_wn:\n",
        "              keywords_wn[keyword]=tfidf[stemlemma(keyword)]\n",
        "            else:\n",
        "              keywords_wn[keyword]=max(keywords_wn[keyword],tfidf[stemlemma(keyword)])\n",
        "      else:\n",
        "        keywords_wn[keyword]=0\n",
        "\n",
        "  if K and len(tfidf)>0:\n",
        "    return getTopK(keywords_wn,K=K)\n",
        "\n",
        "  else:\n",
        "    return list(keywords_wn.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erCm2ORN6tY7"
      },
      "source": [
        "## Build TFIDF matrix:\n",
        "If you have a set of documents of domains you can build a TFIDF matrix to  enhance the keyword extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjUkq3Eg6VRP"
      },
      "outputs": [],
      "source": [
        "def buildTFIDFvector(docs,use_ngrams=True,ngrams=4):\n",
        "  if use_ngrams:\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,ngrams),min_df=0,stop_words=stopwords.words('english'))\n",
        "  else:\n",
        "    vectorizer = TfidfVectorizer(min_df=0,stop_words=stopwords.words('english'))\n",
        "  vectors = vectorizer.fit_transform(docs)\n",
        "  return pd.DataFrame(vectors.todense().tolist(), columns=vectorizer.get_feature_names_out())\n",
        "def buildTFIDF(domains,files,use_ngrams=True,ngrams=3):\n",
        "  docs={}\n",
        "  for d in domains:\n",
        "    docs[d]=stemlemma(' '.join([files[doc] for doc in domains[d]]))\n",
        "  return buildTFIDFvector(list(docs.values()),use_ngrams=use_ngrams,ngrams=ngrams)\n",
        "\n",
        "def getTFIDFscore(q,id,tfidf):\n",
        "  score=0\n",
        "  for t in q.split():\n",
        "    if t in tfidf[q].columns:\n",
        "      score+=tfidf[q][t][id]\n",
        "  return score  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aZuH6rO63H6"
      },
      "source": [
        "## Get corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQQ6IPSekfkX"
      },
      "outputs": [],
      "source": [
        "def getCorpus(list_of_keywords, title_similarity=False,sim_threshold=0.5,filtered_cats=[], auto_suggest = False, depth=1,verbose=False):\n",
        "  processed_titles=[] # we store processed titles of wikipedia articles to avoid processing them more than once\n",
        "  corpus= [] # here we store extracted articles\n",
        "  for keyword in tqdm(list_of_keywords, disable= verbose):\n",
        "    if not verbose:\n",
        "      print('=== Processing keyword:',keyword, end='\\r')\n",
        "    # we search for the closest titles matching our keyword\n",
        "    if auto_suggest:\n",
        "      matching_titles=wiki.search(keyword,suggestion=True)  \n",
        "      if not matching_titles:\n",
        "        continue\n",
        "      for title in matching_titles:\n",
        "        # you can add a similarity criteria between the keyword and the matching article before proceeding\n",
        "        # for example use jaccard with a threshold: if calculate_jaccard(keyword,title)>0.5\n",
        "        if title not in processed_titles:\n",
        "          if title_similarity:\n",
        "            if calculate_jaccard(title,keyword)<sim_threshold:\n",
        "              continue\n",
        "          corpus.extend(getCorpusFromTitle(title,filtered_cats,depth=depth,verbose=verbose))\n",
        "          title.append(processed_titles)\n",
        "    else:\n",
        "      corpus.extend(getCorpusFromTitle(keyword,filtered_cats,depth=depth,verbose=verbose))\n",
        "  return list(set(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uunoOiwth6u"
      },
      "outputs": [],
      "source": [
        "def getCorpusFromTitle(title,filtered_cats,depth=1,verbose=False):\n",
        "  corpus= [] # here we store extracted articles\n",
        "  page = wp.page(title) # get the page that matches the title\n",
        "  if page:\n",
        "    try:\n",
        "      try:\n",
        "        corpus.append(page.text) # add matched page to the corpus list\n",
        "      except:\n",
        "        corpus.append(wiki.page(wiki.search(title)[0]).content)\n",
        "    except:\n",
        "      None\n",
        "    # browse the categories of the page\n",
        "    try:\n",
        "      cats=page.categories\n",
        "    except:\n",
        "      return corpus\n",
        "    if depth>0:\n",
        "      if len(cats)<50: #max number of categories\n",
        "        for cat_title, category in cats.items(): # There are some generic categories that we want to filter out (e.g, Category:articles from August 2019).\n",
        "          if not match(cat_title, filtered_cats): \n",
        "            if verbose:\n",
        "              print(\"\\t== Extracting articles from the category:\",cat_title)\n",
        "            # depth=1 get all articles in each category, depth=2: include the articles in subcategories, depth=3: include the articles in subsubcategories. \n",
        "            corpus.extend(get_articles_in_category(category,filtered_cats, max_level=depth,verbose=verbose)) \n",
        "  return list(set(corpus))\n",
        "\n",
        "def match(title,filters):\n",
        "  for filter in filters:\n",
        "    if filter.lower() in title.lower():\n",
        "      return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjFDUKKyqQ0j"
      },
      "outputs": [],
      "source": [
        "def get_articles_in_category(category,filtered_cats, level=1, max_level=2,maxlimit=200, verbose=False):\n",
        "  articles=[]\n",
        "  try:\n",
        "    categorymembers=category.categorymembers\n",
        "    if len(categorymembers)>maxlimit:\n",
        "      return articles\n",
        "    for cat_title, c in tqdm(categorymembers.items(), disable=not verbose):\n",
        "\n",
        "      if c.ns != wikipediaapi.Namespace.CATEGORY:\n",
        "        try:\n",
        "          articles.append(c.text)\n",
        "        except:\n",
        "          articles.append(wiki.page(cat_title).content)\n",
        "      elif level < max_level and not match(cat_title, filtered_cats):\n",
        "          articles.extend(get_articles_in_category(c,filtered_cats, level=level + 1, max_level=max_level))\n",
        "  except Exception as e: \n",
        "    print(e)\n",
        "  return articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z8sRfmvglrT"
      },
      "outputs": [],
      "source": [
        "def saveCorpus(docs,parent_dir,folder='Corpus'):\n",
        "  for i in range(0,len(docs)):\n",
        "    doc=docs[i]\n",
        "    path = os.path.join(parent_dir, folder)\n",
        "    os.mkdir(path)\n",
        "    filename='doc'+str(i)+'.txt'\n",
        "    filepath = os.path.join(path, filename)\n",
        "    text_file = open(filepath, \"w\")\n",
        "    n = text_file.write(doc)\n",
        "    text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHF97QMGYiF1"
      },
      "outputs": [],
      "source": [
        "def createWordCloud(corpus,image_name='Word Cloud'):\n",
        "  WC=WordCloud(stopwords=set(nlp.Defaults.stop_words), #width = 1000, height = 500,\n",
        "                        max_font_size=50, max_words=100,background_color=\"white\")\n",
        "  wordcloud = (WC.generate(' '.join(corpus)))\n",
        "\n",
        "  plt.figure(figsize=(15,8))\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.savefig(image_name+\".png\", bbox_inches='tight')\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBEpLA8eYljB"
      },
      "outputs": [],
      "source": [
        "def getTotal_nbr_words(corpus): \n",
        "  total_nbr_words=0\n",
        "  for article in corpus:\n",
        "    total_nbr_words+= len(word_tokenize(article))\n",
        "  print(\"total number of words:\", total_nbr_words)\n",
        "  return total_nbr_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiUJMTFnYpDq"
      },
      "outputs": [],
      "source": [
        "def readPDF(file):\n",
        "  reader = PdfReader(file)\n",
        "  return ' '.join([re.sub(r\"\\s+\", \" \",page.extract_text().replace('\\n',' ')).strip() for page in reader.pages])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_cFSw8nknrA"
      },
      "source": [
        "Similarity check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simCheck(doc,corpus,nlp=nlp):\n",
        "  doc = nlp(doc)\n",
        "  c=[]\n",
        "  for article in corpus:\n",
        "    sim=doc.similarity(nlp(article))\n",
        "    if sim>0:\n",
        "      c.append(sim)\n",
        "  # print(min(c),np.average(c),max(c))\n",
        "  return min(c),np.average(c),max(c)"
      ],
      "metadata": {
        "id": "Q4bEG7yThvjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simCheckV2(doc1,doc2,corpus,nlp=nlp):\n",
        "  doc1 = nlp(doc1)\n",
        "  doc2 = nlp(doc2)\n",
        "  c1=[]\n",
        "  c2=[]\n",
        "  for article in tqdm(corpus):\n",
        "    article=nlp(article)\n",
        "    sim1=doc1.similarity(article)\n",
        "    sim2=doc2.similarity(article)\n",
        "    if sim1>0:\n",
        "      c1.append(sim1)\n",
        "    if sim2>0:\n",
        "      c2.append(sim2)\n",
        "  c3=c1+c2\n",
        "  # print(min(c1),np.average(c1),max(c1))\n",
        "  # print(min(c2),np.average(c2),max(c2))\n",
        "  print(min(c3),np.average(c3),max(c3))"
      ],
      "metadata": {
        "id": "oT6fCFQfYKa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdruZw8xkr4-"
      },
      "outputs": [],
      "source": [
        "def docSimilarity(doc1,doc2,nlp):\n",
        "  return nlp(doc1).similarity(nlp(doc2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzKj0AbqzU4n"
      },
      "source": [
        "# Usage example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQvQVozN-aJR"
      },
      "source": [
        "creating some rules to exclude generic categories based on our observations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filters=['Articles','Commons','WikiData','Wikipedia','Webarchive','disputes','bot:','CS1','errors','pages','births','deaths','disambiguation','elements']+[calendar.month_name[i] for i in range(1,12)]+[calendar.month_abbr[i] for i in range(1,12)]"
      ],
      "metadata": {
        "id": "rhVRKgdJi68a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's download the PURE dataset"
      ],
      "metadata": {
        "id": "80Lny5YrgC3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://zenodo.org/record/1414117/files/requirements.zip\n",
        "!unzip -q requirements.zip"
      ],
      "metadata": {
        "id": "OmzD4EblfNfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We experiment with these documents from **railway** and **transportation** domains (according to this table http://nlreqdataset.isti.cnr.it/)"
      ],
      "metadata": {
        "id": "GaVdln6rgiyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs={'railway':['2007 - ertms.pdf','2006 - eirene sys 15.pdf','2007 - eirene fun 7.pdf'],\n",
        "      'transportation':['2001 - ctc network.pdf','2005 - pontis.pdf','2007 - mdot.pdf']}"
      ],
      "metadata": {
        "id": "NFEUfSbte_Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files={doc:readPDF('req/'+doc) for docset in docs.values() for doc in docset}"
      ],
      "metadata": {
        "id": "eFgYqzAkg_EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf=buildTFIDF(docs,files)"
      ],
      "metadata": {
        "id": "vbgS3hOtwzGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_ids={'railway':0,'transportation':1}"
      ],
      "metadata": {
        "id": "exDxgLPLyp-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def saveObj(ob,filename):\n",
        "    filehandler = open(filename, 'wb') \n",
        "    pickle.dump(ob, filehandler)\n",
        "    filehandler.close()"
      ],
      "metadata": {
        "id": "_q01N5ZiGCpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZcOXVSpDBU6"
      },
      "source": [
        "NOTE: This step takes time to run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords_set={}\n",
        "corpora={}\n",
        "for dom,v in docs.items():\n",
        "  print('#### domain:'+dom)\n",
        "  docname=v[0]\n",
        "  print('# doc:'+docname)\n",
        "  doc=files[docname]\n",
        "  keywords = getKeywords(doc,nlp,include_nouns=True,tfidf=tfidf.loc[tfidf_ids[dom]],K=50) # extract keywords\n",
        "  keywords_set[docname]=keywords\n",
        "  print(len(keywords),' keywords extracted ####')\n",
        "  corpus = getCorpus(keywords, title_similarity=True, filtered_cats=filters, auto_suggest = False, depth=1,verbose=True)\n",
        "  saveObj(corpus,dom+'.corpus')\n",
        "  corpora[docname]=corpus\n",
        "  print('### number of articles:',len(corpus))\n",
        "  simCheckV2(files[v[1]],files[v[2]],corpus)\n",
        "  getTotal_nbr_words(corpus)\n",
        "  createWordCloud(corpus,dom+'WC')"
      ],
      "metadata": {
        "id": "qi_ygVLMhjzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saveObj(corpora,\"corpora.obj\")"
      ],
      "metadata": {
        "id": "gPdyikfAqsYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS7krqn6zd2S"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "WikiDoMiner.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}